# 66 Days Of Data<br>

âš¡ Tracking my #66daysofdata challenge<br>

âš¡ Follow me on Twitter for live updates: https://twitter.com/veroanalytic<br>

---

### Day 23 | Thursday | September 29, 2022

**Today's Progress**:
Briefly touched on recursive functions in Python, but mainly continued on with practice in handling of missing data with Pandas.


**Thoughts:**
Spent time today getting my teammates at work access to Anaconda. I'm really excited about this because the more my team gets involved with using Python, the more I see it becoming a regular skillset to solve for our different business problems.

Meaning a better chance at accelerating my Python skills through actual work application!


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 22 | Wednesday | September 28, 2022

**Today's Progress**:
More data importing, plotting, and cleaning with pandas. All still lesson based, but picking up good bits that I'll be using on my weekend projects.



**Thoughts:**
Well I didn't wake up early, and I still have low motivation to journal here lol.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 21 | Tuesday | September 27, 2022

**Today's Progress**:

Back to more Python learning.

Hoping to wrap up this short course by next Wednesday, but the pace I'm going is slower and more deliberate to make sure I'm doing all the exercises to reinforce my comprehension.


**Thoughts:**

Tired brain after a long day. I'm wondering if I should somehow move this journaling to the morning of the next day to write something more substantial after having a nights rest.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 20 | Monday | September 26, 2022

**Today's Progress**:
- Learned to use SQL Server Data Tools (SSDT) in Visual Studio, to create a SSIS package that would load CSV files into a SQL table.
  - Utilizing a foreach loop container allows the package to load similar formatted files that are stored in the same location.
- Began going over Pandas data frame in the tutorial I am following. I will be paying close attention here to strengthen my foundational knowledge of Pandas as it is my most used library so far as a data analyst.


**Thoughts:**
Slowly but surely, I am integrating tools into my day-to-day that I previously relied on other to execute. Only scratching the surface, but I will be making this a constant effort to become a more proficient analyst.

**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 19 | Sunday | September 25, 2022

**Today's Progress**:
- Began data exploration and cleaning of a survey dataset that has captured responses of a person's behavioral health, physical health, and a multitude of other conditions.
- Completed Numpy lecture and exercises and have moved on to the Pandas lecture
  - First part of the lecture went over ways to create, index, and manipulate Pandas Series
- Worked on my resume and made quite a few changes. One more evening working on it and I think it should be completed


**Thoughts:**
Today was much more productive than yesterday. I started with a grabbing a new dataset to analyze and begin trying to form a standardized and reusable approach to data analysis. Later on the evening I resumed my guided tutorial of python data analysis. Finally, I wrapped up the evening sprucing up my resume.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- BRFSS analysis: https://github.com/veroanalytic/analytics-template

---

### Day 18 | Saturday | September 24, 2022

**Today's Progress**:
Did some HackerRank exercises and finished the first draft of my resume.



**Thoughts:**
Today was a busy day outside so my attention was not fully on studying.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data


---

### Day 17 | Friday | September 23, 2022

**Today's Progress**:
Didn't work on anything data analytics related, but I did work on sprucing up my resume.


**Thoughts:**
So I've been working as a data analyst for about a year in my current company. Previously, I worked as a systems analyst for over four years in the same company.

However, I was recently informed that a position has opened up in my company, which would be a promotion to my current role. The caveat being, I may not necessarily be functioning as a data analyst, but more as a business consultant.

In such a large organization, the titles of positions are very fluid and can mean different things depending on which department you are in. Still, this opportunity is enticing enough for me to take the steps to prepare for a potential interview.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data


---

### Day 16 | Thursday | September 22, 2022

**Today's Progress**:
- More Numpy review. Hoping to spend time this weekend away from tutorial watching land and back to working on data analysis projects.



**Thoughts:**
Still finding very low motivation in doing any additional journaling here, but that's okay. There's always tomorrow.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data


---

### Day 15 | Wednesday | September 21, 2022

**Today's Progress**:
- Currently on part 4 of the python analysis tutorial, which is focused on Numpy
  - I reviewed vectorized operations as well as the handling and slicing of matrices
- I also ran into an issue where Pandas couldn't handle updating the date format for the following value: '9999-12-31'.
  - After much googling, I finally realized I could just handle the date conversion on SQL before I pull the data into my dataframe. Problem solved lol.



**Thoughts:**
Depending on the day, it gets really difficult to motivate myself to update this log. The amount of details I share vary depending on that motivation.



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data


---
### Day 14 | Tuesday | September 20, 2022

**Today's Progress**:
- Built a Python program at work today that would generate a dataframe with mocked up member demographic information (name, gender, address, census tract, etc.)
  - The process randomizes all of these fields each time it's run, and extracts the df into a csv.
  - Since this is being done on my work computer, I am limited in the libraries that I can use to generate random values
  - Currently going through sFTP testing with an external vendor, and with this program I'm able to automate the randomizing of member level detail, scale the number of records easily (100k records ran in one second), and generate a unique file each time it is run.
- Continued on with data analysis python tutorial
  - Completed part 3, and learned about Bokeh, which is a library that generates interactive plots. I'll need to play around with this more
  - Moved on to part 4 which is focusing on Numpy. So far I learned that Numpy has built a ton of efficiencies in handling numeric data in comparison to base Python (due to Python being a high-level language).


**Thoughts:**
When I received an email from my director to mock up a test file for vendor testing, I knew right away I would want to do this in Python.

Due to the time-sensitivity, I manually mocked up the first one to get the ball rolling. After that, I built out my Python program piece by piece.

This approach paired with lots and lots of googling got me to a finished product that will allow me to generate as many mocked up files as needed.

I think tomorrow I'm going to test the limits on how big of a file I can export out of the dataframe I am creating in the program.

More and more I'm starting to feel like an actual Python programmer, which is an amazing feeling. I will continue to look for areas that Python will improve my day-to-day, but also I will need to find avenues that I can conduct analysis in Python that will be valuable to my stakeholders.

One step at a time.

**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---
### Day 13 | Monday | September 19, 2022

**Today's Progress**:
- Currently on part 3 of the "Data Analysis with Python" tutorial
  - It's going over Jupyter Notebooks, which I am already familiar with, so I was watching this part at 1.5x speed lol
  - Luckily, I didn't skip through it because it started going over a cryptowatch API, which I am always up for learning more API's related to crypto that I can tap in to.
- I also spent time today at my job and methodically chose a simple report that I'm responsible for, and was able to automate the formatting and exporting to Excel.



**Thoughts:**
Automating a report at my job through Python is huge for me. I had a goal of integrating Python into my day-to-day responsibilities by November, and I have already achieved a small part of that goal.

This is just the beginning. I will keep looking for other ways to improve my work with the use of Python.

I'll also soon be looking into some Tableau upskilling so that I can provide that value to my business stakeholders as well without having to rely on my Tableau engineer teammate.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---
### Day 12 | Sunday | September 18, 2022

**Today's Progress**:
- Began exploratory analysis of a CMS health insurance dataset
- Started a data analysis Python course on YouTube.
  - Completed ch1 and ch2 exercises from the "Data Analysis with Python" tutorial


**Thoughts:**
So far, I'm enjoying the YouTube tutorial I am following, while also balancing with analysis on other datasets, like the one I found in Kaggle for CMS health insurance.

The YouTube tutorial has a lot of different examples through each lecture to reinforce what had been discussed. I think they've been very effective, and even in my own separate analysis, I am recalling instances from the tutorial that apply to that analysis.

With continuous repetition in Python coding, analytical thinking, and dedicated consistency, I really am enthusiastic that I am honing skills that I will be able to integrate and persist in my career.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- Kaggle Notebook - Health Insurance Analysis: https://www.kaggle.com/code/veroanalytic/health-insurance-marketplace-analysis/notebook


---
### Day 11 | Saturday | September 17, 2022

**Today's Progress**:
- Worked on the CoinMarketCap web scraper project
  - For some reason, my Jupyter kernel was dying whenever I ran the automation portion of the code, so I had to comment it out and run the web-scraper function manually
  - Even so, moved on and added a process to import the data scraped into a CSV
  - Did some data cleaning and also some data visualizations of cryptocurrency price changes
    - Used the stack() function to get the data set into a structure that could be better visualized.
- Also resumed reading of "Automate the Boring Stuff with Python." Currently on ch 5.
- Working through a Data Analysis with Python tutorial by freeCodeCamp.org, on Youtube:
  - https://www.youtube.com/watch?v=r-uOLxNrNk8&list=PLNHegA0hHsNr0U6mnFBIfAS8l2u8P_WK9&index=2&t=151s&ab_channel=freeCodeCamp.org


**Thoughts:**
Fought the urge to sign up for Google's Data Analytics certificate on Coursera.

Further along in my journey, I will probably give it a shot to at least get the certificate, and to also enhance my skillset with R, the programming language chosen for the certificate.

For now, I am remaining consistent in Python learning/coding and data analysis thinking.



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- CoinMarketCap Web Scraper, Data Cleaning, and Data Viz repo: https://github.com/veroanalytic/coinmarketcap-automated.git

---
### Day 10 | Friday | September 16, 2022

**Today's Progress**:
- Not currently at home, so I couldn't work on my usual slew of projects. Instead did some HackerRank Python exercises.


**Thoughts:**
I'm finding it difficult to solve many of these HackerRank Python exercises. What I am hoping is by doing them and then documenting out how and why the problem was solved will help me comprehending what is logically going on.



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- HackerRank repo: https://github.com/veroanalytic/hacker-rank-python.git


---
### Day 9 | Thursday | September 15, 2022

**Today's Progress**:
Still exploring different avenues of pulling data.
- Leveraged @CoinMarketCap API to pull cryptocurrencies of interest.
- Thinking through different ways of staging and then analyzing the data

Not enough time today, but more to come.


**Thoughts:**
Bit by bit, every mini-project is starting to have areas of convergence. I will soon document how and what I would like to intersect for a much larger effort.

I think I would also like to start blogging my experiences in this challenge as of now.
Maybe do a bi-weekly retrospective.

I need to find the time to dedicate to writing, which I worry will knock me off track of my goals with Python and data analysis.



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- Amazon Web Scraper: https://github.com/veroanalytic/web-scraper-amazon.git
- CoinMarketCap Web Scraper: https://github.com/veroanalytic/coinmarketcap-automated.git


---
### Day 8 | Wednesday | September 14, 2022

**Today's Progress**:
Working on a web scraping project.
- The barebone functionality is working as intended, but I need to spend some serious time refining what I would truly like to scrape, and what to do with the data once I have it.
Late night coding:
- I have requested a developer account for CoinMarketCap to have access to their API
  - I have successfully pulled data from CMC into a dataframe using their pre-configured API start code for Python

**Thoughts:**
Fighting off the urge to buy another course that I won't end up completing. Instead I'm thinking through a framework of small projects I've done, to dedicate to into one singular, large project endeavor.
- Web Scrape an area of interest
- Store this dataset somewhere
- Explore the dataset (are there columns of interest?)
  - Start thinking through questions that I would like the data to answer from these columns
  - Come up with ideas and choose one or two you'd really like to answer
  - Clean as needed
- Come up with a plan to accomplish one of those ideas
- Run stats off of the data, and visualize it
  - Answers to those earlier questions should be found here



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- Amazon Web Scraper: https://github.com/veroanalytic/web-scraper-amazon.git
- CoinMarketCap Web Scraper: https://github.com/veroanalytic/coinmarketcap-automated.git


---
### Day 7 | Tuesday | September 13, 2022

**Today's Progress**:
- Still taking a break from the Monkeypox analysis, to work on a movie industry dataset
  - Completed the correlation analysis of this movie industry dataset


**Thoughts:**
Changing things up by analyzing a movie industry dataset from Kaggle. I am throwing in a mix of guided projects to bolster the way I think through analyzing different types of datasets and use cases.

**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- movie-industry-analysis: https://github.com/veroanalytic/movie-industry-analysis.git


---
### Day 6 | Monday | September 12, 2022

**Today's Progress**:
- Received some good feedback on the Monkeypox analysis that I have shared thus far. (Thanks https://twitter.com/meyke9976 ðŸ˜Š)
  - I will work on trying to normalize aspects of the data for the next time I share any results.
  - For example, hospital bed availability being a factor in hospitalizations of Monkeypox cases.
  - Also, still working on cleaning symptom types in the dataset, and considering the dataset is updated daily, I will need to do a pull of the new data and rerun my analysis up to this point.
- Continued with more Monkeypox analysis after work, focusing on travel history:
  - There seems to be very limited evidence of travel history in relation to total cases (less than 1%)
  - Top 5 countries with attested travel history (by raw counts): U.S, Germany, Portugal, Brazil, Italy
  - Taking the ratio of travel history by total cases, Portugal and Italy have the highest rate of cases
  - Portugal: Travel History: 34 / Total Cases: 871
  - Italy: Travel History: 23 / Total Cases: 805
  - Countries with low volume of confirmed cases heavily skew the analysis
- As a late night session, I decided to start doing exploration of another dataset. This time a movie industry dataset.
  - I'll be following along with Alex The Analyst's Portfolio project on YouTube: https://www.youtube.com/watch?v=iPYVYBtUTyE&list=PLUaB-1hjhk8H48Pj32z4GZgGWyylqv85f&index=5&t=215s&ab_channel=AlexTheAnalyst

**Thoughts:**
Headed to the gym, will reflect after.

Still spending several hours a day on doing analytics in VS Code (Jupyter Notebooks plugin), and having a blast. I feel repetition is key to being able to building a efficient and sustainable approach to analysis.

Definitely still in the beginning stages, but I am hopeful to become more proficient with time.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- monkey-pox-analysis: https://github.com/veroanalytic/monkey-pox-analysis.git

---
### Day 5 | Sunday | September 11, 2022

**Today's Progress**:
- Worked on submodules to copy repos I've worked on during this challenge, into my 66-days-of-data repo
  - I'll continue to work on this to clone other repos into my 66-days-of-data repo to track all the projects and progress dedicated to this round of the challenge
- Still working with GH Pages. I need to figure out how to configure the DNS between GH Pages to my personal domain: http://www.veroanalytic.com/
- Continued working on analysis of Monkeypox cases worldwide. A few observations:
  - The U.S has the highest reported cases by 3x
  - Germany and Italy have the highest hospitalizations
  - Italy leads the group at 2.2% of cases leading to hospitalization
- Worked on cleaning up the "Symptoms" classification of Monkeypox
  - There are a lot of inconsistencies in the data, so I am attempting to transform similar named symptoms into a shared category to be able to better illustrate which symptoms are most prevalent when contract Monkeypox
    - Used regex to match to the particular symptoms
- Did more data cleaning of the Monkeypox dataset. In particular, I worked on conforming similar symptoms to a standardized naming convention.
  - Flu-like symptoms and different forms of skin lesions/ulcers are prevalent.


**Thoughts:**
Spent several hours, in different chunks throughout the day. I am really enjoying doing data analytics in VS Code (Jupyter Notebooks).

I do need to come back and continue my studies of "Automating the Boring Stuff with Python", but for now it will have to "remain on the shelf" so to speak.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- monkey-pox-analysis: https://github.com/veroanalytic/monkey-pox-analysis.git
- GH Page: https://veroanalytic.github.io/


---
### Day 4 | Saturday | September 10, 2022

**Today's Progress**:
- Decided to skip a head a few chapters, and currently reviewing chapter 9, "Reading and writing files"
  - Learned about the pathlib module, and how it is better practice to use forward slashes in your code, even when working in Windows because it will allow your code to be able to run on other operating systems like macOS and Linux
- Conducted analysis on a worldwide dataset of Monkeypox cases, and the timeline of cases being reported
- Set up GitHub Pages for the first time. I will be using this to host a custom domain.

**Thoughts:**
I enjoyed working within Jupyter Notebooks again. It's definitely been a while. I think I'll continue working on the analytics portion of Python programming instead of the conceptual studying.


**Link to work:**
- monkey-pox-analysis: https://github.com/veroanalytic/monkey-pox-analysis.git

---
### Day 3 | Friday | September 9, 2022

**Today's Progress**:
- Still continuing with Lists from "Automating the Boring Stuff with Python". This feels like a long chapter lol.
  - Reviewed tuples, and converting types with the list() and tuple() function
  - Clarified that variables do not necessarily store values, but they store 'references' to values. These references are stored in the computers memory (think a container or box)
  - Noted the id() function that returns the identity of the variable reference
  - Worth noting again, list methods like append() modify 'in-place'. So if my_list = ['cat', 'dog'] and has an ID of 123, if I run my_list.append('squirrel'), my_list is modified in place, and still contains the same reference ID of 123
- ** placeholder until I resume my studies. maybe after work
  - Studying has continued after dinner, where I learned that when using copy() and deepcopy() functions, you are creating an actual duplicate copy of a mutable value like a list; you are not just copying the reference of the list. Meaning the variable holding the original list, and the variable holding the copied list are independent of each other
  - Followed the code to create a 'Conway's Game of  Life' short program. Honestly it's over my head right now, but I'll have this saved away to refer to if needed.



**Thoughts:**
Unexpectedly woke up at 6 am, but instead of trying to force myself to go back to sleep, I decided to get up and have an early start to my studies.

This has been a goal for myself for a while to be able to wake up early and code/study before work. Here's hoping I can do this consistently and make it a habit.

**Thoughts: Late night**
I may make the decision to jump around in the chapters for Automating the Boring Stuff with Python. Conceptually, there's a lot of information, but pertaining to what I want to accomplish at my job, I don't think I need all of the details that are being depicted in some of the chapters.

I may start hopping around more towards chapters relating to actual automation, and then retroactively go back at chapters to understand any concepts. I think this will help it stick better.


**Link to work:**
- About Me: https://github.com/veroanalytic
- auto-boring-stuff repo: https://github.com/veroanalytic/auto-boring-stuff.git

---
### Day 2 | Thursday | September 8, 2022

**Today's Progress**:
- Updated my GitHub overview page with an About Me: README.md file. I used a generator to get the template, but will continue to alter it as needed.
- Still continuing with lists from "Automating the Boring Stuff with Python"
  - Reviewed list methods like append() and insert()
  - Also learned about the enumerate function
- ** putting a place holder here because I will resume studies after the gym :D
- Got back from the gym and continued more through my review of lists
  - practiced list methods such as remove(), sort(), passed keyword arguments within these methods like sort(reverse=True), to sort in descending order.
  - Went over sequence data types: lists, strings, range objects returned by the range() function and tuples are considered sequence data types. Meaning they can be indexed, sliced, and used in for loops with len() or with in and not in operators.


**Thoughts:**
I'm feeling impatient and want to get through these chapters that are of review for me and to the later chapters dealing with actual Python automation.

But I little by little, there are additional things that I'm picking up that I either forgot, or never learned during my initial pass at learning Python.

This is a marathon not a sprint, so I will continue to take it slower but with consistency.

**Thoughts after the gym:** I think I like this routine.
- Finish work
- Have dinner
- Study for a bit, digesting food, and record #66daysofdata challenge
- Go to gym
- Continue studying until tired

We'll see if I can keep this up.


**Link to work:**
- About Me: https://github.com/veroanalytic
- auto-boring-stuff repo: https://github.com/veroanalytic/auto-boring-stuff.git

---
### Day 1 | Wednesday | September 7, 2022

**Today's Progress**:
- Created 66-days-of-data repo to track progress on this challenge.
- Used Git to clone the remote repo down to my local repo. Pushing changes back to GitHub through Git as well.
- Continuing Chapter 4 of "Automating the Boring Stuff with Python", which is focused on lists.
  - Working with lists:
    - Created a variable and assigned it as an empty list []
    - Followed that with a while loop that breaks out of the loop of an empty string is entered
    - Otherwise it continues iterating and allows for more inputs which gets outputted in a for loop
    - File name 'allMyCats2_ch4.py' in the auto-boring-stuff.git repo

**Thoughts:**
I took quite a long break from my last round of #66daysofdata. I'm riding this wave of motivation to reestablish this as a habit. My primary goal for this round is to increase my skills with Python.

**Link to work:**
- auto-boring-stuff repo: https://github.com/veroanalytic/auto-boring-stuff.git
