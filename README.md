# 66 Days Of Data<br>

⚡ Tracking my #66daysofdata challenge<br>

⚡ Follow me on Twitter for live updates: https://twitter.com/jasonvero<br>

---

### Day 74 | Sunday | November 27, 2022

**Today's Progress**:



**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---


### Day 73 | Saturday | November 26, 2022

**Today's Progress**:
Took a break from journaling. Need to get back to this.

Worked on my Financial transaction portfolio on Google Sheets. Replaced a few hardcoded values with XLOOKUP formulas.


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 72 | Friday | November 18, 2022

**Today's Progress**:
Optimized the my daily_data df to only include necessary columns, which improved overall performance of my financial-portfolio app refresh.


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 71 | Thursday | November 17, 2022

**Today's Progress**:
Studying different variations of multiples analysis to evaluate the valuation of a stock. 

First up is the P/E ratio: Share price / Earnings Per Share  (EPS)

Trailing P/E utilizes historical earnings data for its EPS. 

Forward P/E takes expected earnings for its EPS.

Tomorrow, I'll pull a few more companies in related sectors to review P/E across multiple industry peers.

**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 70 | Wednesday | November 16, 2022

**Today's Progress**:
Took off this evening. Will work on my financial app tomorrow. Still need to work on converting the lastdividenddate column into datetime instead of seconds.

**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 69 | Tuesday | November 15, 2022

**Today's Progress**:
Definitely noticing some coupling issues in my financial portfolio app.
After retroactively renaming a few columns, I had to pinpoint several different modules where the columns were hardcoded 
- Need to look into approaches to prevent this from happening.

**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 68 | Monday | November 14, 2022

**Today's Progress**:
Worked more with my yfinance library. There's just so many different methods to explore, I can't seem to put it down.

After merging a couple of dataframes, I pulled the data into Streamlit with a few conditional statements to denote different colors based on percent change.

**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---


### Day 68 | Sunday | November 13, 2022

**Today's Progress**:
Worked more on my stock tracking project.
- Worked on decoupling and separating tasks that were consolidated in one main function into multiple smaller functions.

**Thoughts:**
More code refactoring. 
- While building my projects up, I start in Jupyter notebooks.
- But D.R.Y principle is typically not followed during this process.
- Eventually, I copy the code to a .py file to find areas to optimize design.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 67 | Saturday | November 12, 2022

**Today's Progress**:
Having some fun with Streamlit.

Using Pandas to import yfinance data on stocks of interest, exporting that data into a .csv, and then writing it into Streamlit.

Created a few derived columns based on rolling average calculations for different time intervals.

**Thoughts:**
Though my challenge is over, I still am enjoying the habit of self study, practice, and just being able to build things.

With that in mind, I will continue to track those efforts on this readme.md file so that I can refer to it in the future if needed.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 66 | Friday | November 11, 2022

**Today's Progress**:
And with that, this challenge is complete. Admittedly, the latter half of the challenge saw a steep drop in length of study/practice.

Either way, I've accomplished my primary goal, which was to integrate Python automation to optimize my day-to-day job.


**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 65 | Thursday | November 10, 2022

**Today's Progress**:
Did more work on my investment portfolio project. Hand picked a few stocks that I would like to track percent changes over different time intervals.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 64 | Wednesday | November 9, 2022

**Today's Progress**:
Dealing with data being transformed incorrectly. I.e., leading zeros from a .CSV format being dropped when being converted from a .TXT pipe delimited format.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 63 | Tuesday | November 8, 2022

**Today's Progress**:
Read the following article and learned about the aggregate function, STRING_AGG(), as well as ROLLUP() which is a sub-clause of GROUP BY.

https://towardsdev.com/useful-database-sql-tricks-for-data-engineering-83fa671cc1d7

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 62 | Monday | November 7, 2022

**Today's Progress**:
Reviewing the differences between regular methods in a function versus class methods:

- Regular methods take the instance as the first argument, passed as "self".
- Class methods take the class as the first argument, passed as "cls". gaining or losing value.


**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 61 | Sunday | November 6, 2022

**Today's Progress**:
Worked on conditional statements to highlight areas in my investment allocations that are gaining or losing value.


**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 60 | Saturday | November 5, 2022

**Today's Progress**:
Working on an investment portfolio app.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 59 | Friday | November 4, 2022

**Today's Progress**:
Refactor a SQL script to combine three report extracts into one.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 58 | Thursday | November 3, 2022

**Today's Progress**:
Refactor a SQL script to combine three report extracts into one.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---


### Day 57 | Wednesday | November 2, 2022

**Today's Progress**:
Learned differences between Python class variables and instance variables, with specific use-cases where you might favor one over the other.
https://www.youtube.com/watch?v=BJ-VvGyQxho&ab_channel=CoreySchafer

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 56 | Tuesday | November 1, 2022

**Today's Progress**:

Was really close to skipping today just due to sheer laziness. Decided against it, and instead finished watching this video on Python classes and instances.

https://www.youtube.com/watch?v=ZDa-Z5JzLYM&t=3s&ab_channel=CoreySchafer


**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 55 | Monday | October 31, 2022

**Today's Progress**:
Feeling a little burnt out with Python studies. Instead, I read an article instead on SQL windows functions. I'm using these more and more in my day to day work, so it's always good to refresh the different applications of each.

https://hipiyushjain.medium.com/sql-windows-functions-explained-like-you-are-5-af6f034fa7d7

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 54 | Sunday | October 30, 2022

**Today's Progress**:
Worked mostly with markdown today.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 53 | Saturday | October 29, 2022

**Today's Progress**:
Reviewed Streamlit documentation.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 52 | Friday | October 28, 2022

**Today's Progress**:
Spent the evening working with SQL. Utilizing the LAG window function to create a column that takes the difference of the the previous row’s aggregated value.

I needed a way to represent the amount of product being accrued for a given week.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---


### Day 51 | Thursday | October 27, 2022

**Today's Progress**:
Taking a break from software design strategy studies and decided to go back and bolster my comprehension around object oriented programming, starting with Classes.

Following along to Corey Schafer's series on YouTube:  https://www.youtube.com/playlist?list=PL-osiE80TeTsqhIuOqKhwlXsIBIdSeYtc

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---


### Day 50 | Wednesday | October 26, 2022

**Today's Progress**:
16 more days left! It's been a very rewarding challenge thus far and I'm glad I revisited it.

For this evening, I finished reviewing the Observer Pattern strategy, leveraging event systems to help break out modules and functions into scalable tasks.

Arjan does a great job at presenting this lesson and giving you an example to follow along with.

https://www.youtube.com/watch?v=oNalXg67XEE&list=PLC0nd42SBTaNuP4iB4L6SJlMaHE71FG6N&index=5&ab_channel=ArjanCodes

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---


### Day 49 | Tuesday | October 25, 2022

**Today's Progress**:
Working on a project that requires the use of multiple modules, so I'm leveraging Python's package functionality.

Below article briefly goes over differences between modules and packages.

https://www.scaler.com/topics/module-and-package-in-python/


**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 48 | Monday | October 24, 2022

**Today's Progress**:
Studying the "Observer" pattern in Python.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 47 | Sunday | October 23, 2022

**Today's Progress**:
Expanded my calculator program to include a wage calculator.

Combined with the mortgage calculator, I want to know what houses are realistic to go after from a financial perspective, versus which ones would result in being house poor.


**Thoughts:**



**Link to work:**
- financial-calculator: https://github.com/veroanalytic/financial-calculator
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 46 | Saturday | October 22, 2022

**Today's Progress**:
Created a 🏡mortgage calculator which only brought me sadness.

**Thoughts:**



**Link to work:**
- financial-calculator: https://github.com/veroanalytic/financial-calculator
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 45 | Friday | October 21, 2022

**Today's Progress**:
Reviewed the Strategy design pattern:
- Updated a script to have better cohesion by breaking a large function into smaller functions with specific processing strategy tasks
- Designed one with classes and another with just functions
- Used type hints

This is through the completion of the following video by Arjan: https://www.youtube.com/watch?v=WQ8bNdxREHU&list=PLC0nd42SBTaNuP4iB4L6SJlMaHE71FG6N&index=4&ab_channel=ArjanCodes

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 44 | Thursday | October 20, 2022

**Today's Progress**:
Revisited an old net user command I use to run to output all of the active directory groups associated with a user.

I then created a Python script that read the output into a df, cleaned it, and returned if a user belonged to a group of interest.

The purpose of this was to verify the level of security access of multiple users.

Back in the day I used to go through the very tedious and messy process of manually scanning the cmd line output or paste it into an Excel to search for the values I was interested in.

❤️🐍

I also continued with learning about strategy design pattern from Arjan. After this video lesson, I will take a step back and refresh myself with classes, since these are being used very often in these software design videos.

https://www.youtube.com/watch?v=WQ8bNdxREHU&list=PLC0nd42SBTaNuP4iB4L6SJlMaHE71FG6N&index=3&ab_channel=ArjanCodes

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 43 | Wednesday | October 19, 2022

**Today's Progress**:

Pretty unexpected account suspension on my second account where I was only posting updates of my data studies. 😅

Appeal has been put in, but for now I'll start posting my updates to the challenge on this account again.

Spent the evening reviewing the "Strategy" design pattern in Python.
https://www.youtube.com/watch?v=WQ8bNdxREHU&list=PLC0nd42SBTaNuP4iB4L6SJlMaHE71FG6N&index=3&ab_channel=ArjanCodes

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 42 | Tuesday | October 18, 2022

**Today's Progress**:
Watched a quick video around deploying software products faster by optimizing your code for change above all else.

Learning more about your customer, implement features that tell you more about their needs, and continue to iterate as you learn more.


4 steps to work towards optimizing for change:
1. Develop what you want to measure
2. Rely on widely used libraries and tools
3. Design the structure of your software
4. Design for the future

https://www.youtube.com/watch?v=MU20ah5s9ww&ab_channel=ArjanCodes

https://pbs.twimg.com/media/FfZhxyWWQAQrrlg?format=png&name=small

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 41 | Monday | October 17, 2022

**Today's Progress**:
Worked with the principle of Dependency Inversion using abstract base classes in Python.

Creating an abstract base class helps with removing dependency (coupling) between classes, ensuring better design in code reusability.
youtube.com

https://www.youtube.com/watch?v=Kv5jhbSkqLE&list=PLC0nd42SBTaNuP4iB4L6SJlMaHE71FG6N&index=3&ab_channel=ArjanCodes

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 40 | Sunday | October 16, 2022

**Today's Progress**:
Completed watching and practicing through the first video of a series regarding the implementation of better software design in Python.

https://youtube.com/watch?v=eiDyK_ofPPM&list=PLC0nd42SBTaNuP4iB4L6SJlMaHE71FG6N&ab_channel=ArjanCodes

https://pbs.twimg.com/media/FfPYn5QWYAQ3f__?format=png&name=900x900


**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 39 | Saturday | October 15, 2022

**Today's Progress**:
Worked on raising the cohesion and lowering the coupling of a few Python classes.

**Thoughts:**



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 38 | Friday | October 14, 2022

**Today's Progress**:
In an attempt to write cleaner code, I am going through a series on Python software design.

Reviewed concepts of cohesion and coupling, which can be interpreted as how easily can your code be understood, changed, or extended.

https://www.youtube.com/watch?v=eiDyK_ofPPM&list=PLC0nd42SBTaNuP4iB4L6SJlMaHE71FG6N&ab_channel=ArjanCodes


**Thoughts:**
Look into TabPy


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---


### Day 37 | Thursday | October 13, 2022

**Today's Progress**:
Day 37 | #66daysofdata

Finished watching a video called "25 Nooby Pandas Coding Mistakes You Should Avoid".

Lots of good tips in such a short video.

https://youtube.com/watch?v=_gaAoJBMJ_Q&list=WL&index=7&ab_channel=MedallionDataScience


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 36 | Wednesday | October 12, 2022

**Today's Progress**:

Going through a video that goes over beginner mistakes while using Pandas.

It's a short video, but I'm going through it slow and taking notes that I can refer to later.

https://youtube.com/watch?v=_gaAoJBMJ_Q&list=WL&index=7&ab_channel=MedallionDataScience



**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 35 | Tuesday | October 11, 2022

**Today's Progress**:
Brain is mush from reformatting someone's 1000 line SQL script in order to better comprehend what is going on. Subquery after subquery..x10. What a nightmare 😢

Ironically, my Tableau review was focused on joins, which brought me right back into SQL.

Also read this article comparing subqueries vs joins.
- https://www.essentialsql.com/rewrite-subquery-as-join/

**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 34 | Monday | October 10, 2022

**Today's Progress**:
Continued with Tableau and Python this evening.

With Tableau, I explored different types of visualizations and practiced using the Marks section for customization.

For Python, I practiced two different solutions to solve a HackerRank problem: list comprehension / sorted() and set()


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 33 | Sunday | October 9, 2022

**Today's Progress**:
Halfway through the challenge, it feels appropriate to do a retrospective of skills I have gained more Python proficiency with:
- data importing through different file types and API's
- data cleaning and analysis using Pandas
- file generation automation

With this retrospective, I am trying to address what I feel to be one of my biggest mistakes when undertaking this challenge last year. Not taking time to reflect and appreciate the progress being made.


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 32 | Saturday | October 8, 2022

**Today's Progress**:
Worked with list comprehensions on HackerRank.

Admittedly, these past few days I’m not accomplishing as much as I’d like, but with things being as busy as they are I’ll settle with keeping the habit going.


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 31 | Friday | October 7, 2022

**Today's Progress**:
Another late and brief study session. Decided to resume reading of “Automate the Boring Stuff with Python.”
- Reviewed the get() and setdefault() method of a dictionary.


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 30 | Thursday | October 6, 2022

**Today's Progress**:
Moving right along in the Tableau series, I briefly went over the use of bins and calculated fields.


**Thoughts:**
It's been a long day so this was just the little bit of studying I needed to close out the evening.



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---


### Day 29 | Wednesday | October 5, 2022

**Today's Progress**:
Feeling like I've hit a new chapter in this challenge, I'm going to spend this next block of time familiarizing myself with Tableau through @Alex_TheAnalyst's YouTube series.
- https://www.youtube.com/playlist?list=PLUaB-1hjhk8GwbqoVmo_5zuhOa0Tcl3xC


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 28 | Tuesday | October 4, 2022

**Today's Progress**:
Finally finished the Data Analysis with Python course provided by freeCodeCamp.org on YouTube.
- https://www.youtube.com/watch?v=r-uOLxNrNk8&list=LL&index=9&t=1495s&ab_channel=freeCodeCamp.org



**Thoughts:**
With enough base knowledge to undertake tasks and projects, my continued efforts will be assessing when Python can be of use, and then honing in on its implementation.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 27 | Monday | October 3, 2022

**Today's Progress**:
More SQL window functions and Python.
- Created a column in SQL that aggregated the number of times a person appeared in a dataset, and averaged the cost within those different occurrence buckets.
- For Python, I reviewed OOP interfacing with matplotlib.


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 26 | Sunday | October 2, 2022

**Today's Progress**:
Read an article introducing the different types of SQL Windows functions and applicable use cases for each. This was a good review since I will be spending this week  analyzing a small dataset gathered by a new stakeholder.
- https://www.analyticsvidhya.com/blog/2020/12/window-function-a-must-know-sql-concept/


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 25 | Saturday | October 1, 2022

**Today's Progress**:
Picked up from last weekend, cleaning and exploring the survey dataset, Behavioral Risk Factor Surveillance System (BRFSS).

Standardizing data values on columns of interest, and inputting that data into a visual to view trend.


**Thoughts:**


**Link to work:**
- BRFSS GitHub: https://github.com/veroanalytic/analytics-template/tree/main/brfss
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---


### Day 24 | Friday | September 30, 2022

**Today's Progress**:
Starting a weekend project utilizing Python to mock up a csv with elements of: member_id, fname, lname, dob, cost, servicedt, batchid

I'll be loading this into my local SQL environment to do analysis on the data using aggregate and window functions.


**Thoughts:**


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 23 | Thursday | September 29, 2022

**Today's Progress**:
Briefly touched on recursive functions in Python, but mainly continued on with practice in handling of missing data with Pandas.


**Thoughts:**
Spent time today getting my teammates at work access to Anaconda. I'm really excited about this because the more my team gets involved with using Python, the more I see it becoming a regular skillset to solve for our different business problems.

Meaning a better chance at accelerating my Python skills through actual work application!


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 22 | Wednesday | September 28, 2022

**Today's Progress**:
More data importing, plotting, and cleaning with pandas. All still lesson based, but picking up good bits that I'll be using on my weekend projects.



**Thoughts:**
Well I didn't wake up early, and I still have low motivation to journal here lol.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 21 | Tuesday | September 27, 2022

**Today's Progress**:

Back to more Python learning.

Hoping to wrap up this short course by next Wednesday, but the pace I'm going is slower and more deliberate to make sure I'm doing all the exercises to reinforce my comprehension.


**Thoughts:**

Tired brain after a long day. I'm wondering if I should somehow move this journaling to the morning of the next day to write something more substantial after having a nights rest.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 20 | Monday | September 26, 2022

**Today's Progress**:
- Learned to use SQL Server Data Tools (SSDT) in Visual Studio, to create a SSIS package that would load CSV files into a SQL table.
  - Utilizing a foreach loop container allows the package to load similar formatted files that are stored in the same location.
- Began going over Pandas data frame in the tutorial I am following. I will be paying close attention here to strengthen my foundational knowledge of Pandas as it is my most used library so far as a data analyst.


**Thoughts:**
Slowly but surely, I am integrating tools into my day-to-day that I previously relied on other to execute. Only scratching the surface, but I will be making this a constant effort to become a more proficient analyst.

**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---

### Day 19 | Sunday | September 25, 2022

**Today's Progress**:
- Began data exploration and cleaning of a survey dataset that has captured responses of a person's behavioral health, physical health, and a multitude of other conditions.
- Completed Numpy lecture and exercises and have moved on to the Pandas lecture
  - First part of the lecture went over ways to create, index, and manipulate Pandas Series
- Worked on my resume and made quite a few changes. One more evening working on it and I think it should be completed


**Thoughts:**
Today was much more productive than yesterday. I started with a grabbing a new dataset to analyze and begin trying to form a standardized and reusable approach to data analysis. Later on the evening I resumed my guided tutorial of python data analysis. Finally, I wrapped up the evening sprucing up my resume.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- BRFSS analysis: https://github.com/veroanalytic/analytics-template

---

### Day 18 | Saturday | September 24, 2022

**Today's Progress**:
Did some HackerRank exercises and finished the first draft of my resume.



**Thoughts:**
Today was a busy day outside so my attention was not fully on studying.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data


---

### Day 17 | Friday | September 23, 2022

**Today's Progress**:
Didn't work on anything data analytics related, but I did work on sprucing up my resume.


**Thoughts:**
So I've been working as a data analyst for about a year in my current company. Previously, I worked as a systems analyst for over four years in the same company.

However, I was recently informed that a position has opened up in my company, which would be a promotion to my current role. The caveat being, I may not necessarily be functioning as a data analyst, but more as a business consultant.

In such a large organization, the titles of positions are very fluid and can mean different things depending on which department you are in. Still, this opportunity is enticing enough for me to take the steps to prepare for a potential interview.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data


---

### Day 16 | Thursday | September 22, 2022

**Today's Progress**:
- More Numpy review. Hoping to spend time this weekend away from tutorial watching land and back to working on data analysis projects.



**Thoughts:**
Still finding very low motivation in doing any additional journaling here, but that's okay. There's always tomorrow.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data


---

### Day 15 | Wednesday | September 21, 2022

**Today's Progress**:
- Currently on part 4 of the python analysis tutorial, which is focused on Numpy
  - I reviewed vectorized operations as well as the handling and slicing of matrices
- I also ran into an issue where Pandas couldn't handle updating the date format for the following value: '9999-12-31'.
  - After much googling, I finally realized I could just handle the date conversion on SQL before I pull the data into my dataframe. Problem solved lol.



**Thoughts:**
Depending on the day, it gets really difficult to motivate myself to update this log. The amount of details I share vary depending on that motivation.



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data


---
### Day 14 | Tuesday | September 20, 2022

**Today's Progress**:
- Built a Python program at work today that would generate a dataframe with mocked up member demographic information (name, gender, address, census tract, etc.)
  - The process randomizes all of these fields each time it's run, and extracts the df into a csv.
  - Since this is being done on my work computer, I am limited in the libraries that I can use to generate random values
  - Currently going through sFTP testing with an external vendor, and with this program I'm able to automate the randomizing of member level detail, scale the number of records easily (100k records ran in one second), and generate a unique file each time it is run.
- Continued on with data analysis python tutorial
  - Completed part 3, and learned about Bokeh, which is a library that generates interactive plots. I'll need to play around with this more
  - Moved on to part 4 which is focusing on Numpy. So far I learned that Numpy has built a ton of efficiencies in handling numeric data in comparison to base Python (due to Python being a high-level language).


**Thoughts:**
When I received an email from my director to mock up a test file for vendor testing, I knew right away I would want to do this in Python.

Due to the time-sensitivity, I manually mocked up the first one to get the ball rolling. After that, I built out my Python program piece by piece.

This approach paired with lots and lots of googling got me to a finished product that will allow me to generate as many mocked up files as needed.

I think tomorrow I'm going to test the limits on how big of a file I can export out of the dataframe I am creating in the program.

More and more I'm starting to feel like an actual Python programmer, which is an amazing feeling. I will continue to look for areas that Python will improve my day-to-day, but also I will need to find avenues that I can conduct analysis in Python that will be valuable to my stakeholders.

One step at a time.

**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---
### Day 13 | Monday | September 19, 2022

**Today's Progress**:
- Currently on part 3 of the "Data Analysis with Python" tutorial
  - It's going over Jupyter Notebooks, which I am already familiar with, so I was watching this part at 1.5x speed lol
  - Luckily, I didn't skip through it because it started going over a cryptowatch API, which I am always up for learning more API's related to crypto that I can tap in to.
- I also spent time today at my job and methodically chose a simple report that I'm responsible for, and was able to automate the formatting and exporting to Excel.



**Thoughts:**
Automating a report at my job through Python is huge for me. I had a goal of integrating Python into my day-to-day responsibilities by November, and I have already achieved a small part of that goal.

This is just the beginning. I will keep looking for other ways to improve my work with the use of Python.

I'll also soon be looking into some Tableau upskilling so that I can provide that value to my business stakeholders as well without having to rely on my Tableau engineer teammate.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data

---
### Day 12 | Sunday | September 18, 2022

**Today's Progress**:
- Began exploratory analysis of a CMS health insurance dataset
- Started a data analysis Python course on YouTube.
  - Completed ch1 and ch2 exercises from the "Data Analysis with Python" tutorial


**Thoughts:**
So far, I'm enjoying the YouTube tutorial I am following, while also balancing with analysis on other datasets, like the one I found in Kaggle for CMS health insurance.

The YouTube tutorial has a lot of different examples through each lecture to reinforce what had been discussed. I think they've been very effective, and even in my own separate analysis, I am recalling instances from the tutorial that apply to that analysis.

With continuous repetition in Python coding, analytical thinking, and dedicated consistency, I really am enthusiastic that I am honing skills that I will be able to integrate and persist in my career.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- Kaggle Notebook - Health Insurance Analysis: https://www.kaggle.com/code/veroanalytic/health-insurance-marketplace-analysis/notebook


---
### Day 11 | Saturday | September 17, 2022

**Today's Progress**:
- Worked on the CoinMarketCap web scraper project
  - For some reason, my Jupyter kernel was dying whenever I ran the automation portion of the code, so I had to comment it out and run the web-scraper function manually
  - Even so, moved on and added a process to import the data scraped into a CSV
  - Did some data cleaning and also some data visualizations of cryptocurrency price changes
    - Used the stack() function to get the data set into a structure that could be better visualized.
- Also resumed reading of "Automate the Boring Stuff with Python." Currently on ch 5.
- Working through a Data Analysis with Python tutorial by freeCodeCamp.org, on Youtube:
  - https://www.youtube.com/watch?v=r-uOLxNrNk8&list=PLNHegA0hHsNr0U6mnFBIfAS8l2u8P_WK9&index=2&t=151s&ab_channel=freeCodeCamp.org


**Thoughts:**
Fought the urge to sign up for Google's Data Analytics certificate on Coursera.

Further along in my journey, I will probably give it a shot to at least get the certificate, and to also enhance my skillset with R, the programming language chosen for the certificate.

For now, I am remaining consistent in Python learning/coding and data analysis thinking.



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- CoinMarketCap Web Scraper, Data Cleaning, and Data Viz repo: https://github.com/veroanalytic/coinmarketcap-automated.git

---
### Day 10 | Friday | September 16, 2022

**Today's Progress**:
- Not currently at home, so I couldn't work on my usual slew of projects. Instead did some HackerRank Python exercises.


**Thoughts:**
I'm finding it difficult to solve many of these HackerRank Python exercises. What I am hoping is by doing them and then documenting out how and why the problem was solved will help me comprehending what is logically going on.



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- HackerRank repo: https://github.com/veroanalytic/hacker-rank-python.git


---
### Day 9 | Thursday | September 15, 2022

**Today's Progress**:
Still exploring different avenues of pulling data.
- Leveraged @CoinMarketCap API to pull cryptocurrencies of interest.
- Thinking through different ways of staging and then analyzing the data

Not enough time today, but more to come.


**Thoughts:**
Bit by bit, every mini-project is starting to have areas of convergence. I will soon document how and what I would like to intersect for a much larger effort.

I think I would also like to start blogging my experiences in this challenge as of now.
Maybe do a bi-weekly retrospective.

I need to find the time to dedicate to writing, which I worry will knock me off track of my goals with Python and data analysis.



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- Amazon Web Scraper: https://github.com/veroanalytic/web-scraper-amazon.git
- CoinMarketCap Web Scraper: https://github.com/veroanalytic/coinmarketcap-automated.git


---
### Day 8 | Wednesday | September 14, 2022

**Today's Progress**:
Working on a web scraping project.
- The barebone functionality is working as intended, but I need to spend some serious time refining what I would truly like to scrape, and what to do with the data once I have it.
Late night coding:
- I have requested a developer account for CoinMarketCap to have access to their API
  - I have successfully pulled data from CMC into a dataframe using their pre-configured API start code for Python

**Thoughts:**
Fighting off the urge to buy another course that I won't end up completing. Instead I'm thinking through a framework of small projects I've done, to dedicate to into one singular, large project endeavor.
- Web Scrape an area of interest
- Store this dataset somewhere
- Explore the dataset (are there columns of interest?)
  - Start thinking through questions that I would like the data to answer from these columns
  - Come up with ideas and choose one or two you'd really like to answer
  - Clean as needed
- Come up with a plan to accomplish one of those ideas
- Run stats off of the data, and visualize it
  - Answers to those earlier questions should be found here



**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- Amazon Web Scraper: https://github.com/veroanalytic/web-scraper-amazon.git
- CoinMarketCap Web Scraper: https://github.com/veroanalytic/coinmarketcap-automated.git


---
### Day 7 | Tuesday | September 13, 2022

**Today's Progress**:
- Still taking a break from the Monkeypox analysis, to work on a movie industry dataset
  - Completed the correlation analysis of this movie industry dataset


**Thoughts:**
Changing things up by analyzing a movie industry dataset from Kaggle. I am throwing in a mix of guided projects to bolster the way I think through analyzing different types of datasets and use cases.

**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- movie-industry-analysis: https://github.com/veroanalytic/movie-industry-analysis.git


---
### Day 6 | Monday | September 12, 2022

**Today's Progress**:
- Received some good feedback on the Monkeypox analysis that I have shared thus far. (Thanks https://twitter.com/meyke9976 😊)
  - I will work on trying to normalize aspects of the data for the next time I share any results.
  - For example, hospital bed availability being a factor in hospitalizations of Monkeypox cases.
  - Also, still working on cleaning symptom types in the dataset, and considering the dataset is updated daily, I will need to do a pull of the new data and rerun my analysis up to this point.
- Continued with more Monkeypox analysis after work, focusing on travel history:
  - There seems to be very limited evidence of travel history in relation to total cases (less than 1%)
  - Top 5 countries with attested travel history (by raw counts): U.S, Germany, Portugal, Brazil, Italy
  - Taking the ratio of travel history by total cases, Portugal and Italy have the highest rate of cases
  - Portugal: Travel History: 34 / Total Cases: 871
  - Italy: Travel History: 23 / Total Cases: 805
  - Countries with low volume of confirmed cases heavily skew the analysis
- As a late night session, I decided to start doing exploration of another dataset. This time a movie industry dataset.
  - I'll be following along with Alex The Analyst's Portfolio project on YouTube: https://www.youtube.com/watch?v=iPYVYBtUTyE&list=PLUaB-1hjhk8H48Pj32z4GZgGWyylqv85f&index=5&t=215s&ab_channel=AlexTheAnalyst

**Thoughts:**
Headed to the gym, will reflect after.

Still spending several hours a day on doing analytics in VS Code (Jupyter Notebooks plugin), and having a blast. I feel repetition is key to being able to building a efficient and sustainable approach to analysis.

Definitely still in the beginning stages, but I am hopeful to become more proficient with time.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- monkey-pox-analysis: https://github.com/veroanalytic/monkey-pox-analysis.git

---
### Day 5 | Sunday | September 11, 2022

**Today's Progress**:
- Worked on submodules to copy repos I've worked on during this challenge, into my 66-days-of-data repo
  - I'll continue to work on this to clone other repos into my 66-days-of-data repo to track all the projects and progress dedicated to this round of the challenge
- Still working with GH Pages. I need to figure out how to configure the DNS between GH Pages to my personal domain: http://www.veroanalytic.com/
- Continued working on analysis of Monkeypox cases worldwide. A few observations:
  - The U.S has the highest reported cases by 3x
  - Germany and Italy have the highest hospitalizations
  - Italy leads the group at 2.2% of cases leading to hospitalization
- Worked on cleaning up the "Symptoms" classification of Monkeypox
  - There are a lot of inconsistencies in the data, so I am attempting to transform similar named symptoms into a shared category to be able to better illustrate which symptoms are most prevalent when contract Monkeypox
    - Used regex to match to the particular symptoms
- Did more data cleaning of the Monkeypox dataset. In particular, I worked on conforming similar symptoms to a standardized naming convention.
  - Flu-like symptoms and different forms of skin lesions/ulcers are prevalent.


**Thoughts:**
Spent several hours, in different chunks throughout the day. I am really enjoying doing data analytics in VS Code (Jupyter Notebooks).

I do need to come back and continue my studies of "Automating the Boring Stuff with Python", but for now it will have to "remain on the shelf" so to speak.


**Link to work:**
- 66-days-of-data repo: https://github.com/veroanalytic/66-days-of-data
- monkey-pox-analysis: https://github.com/veroanalytic/monkey-pox-analysis.git
- GH Page: https://veroanalytic.github.io/


---
### Day 4 | Saturday | September 10, 2022

**Today's Progress**:
- Decided to skip a head a few chapters, and currently reviewing chapter 9, "Reading and writing files"
  - Learned about the pathlib module, and how it is better practice to use forward slashes in your code, even when working in Windows because it will allow your code to be able to run on other operating systems like macOS and Linux
- Conducted analysis on a worldwide dataset of Monkeypox cases, and the timeline of cases being reported
- Set up GitHub Pages for the first time. I will be using this to host a custom domain.

**Thoughts:**
I enjoyed working within Jupyter Notebooks again. It's definitely been a while. I think I'll continue working on the analytics portion of Python programming instead of the conceptual studying.


**Link to work:**
- monkey-pox-analysis: https://github.com/veroanalytic/monkey-pox-analysis.git

---
### Day 3 | Friday | September 9, 2022

**Today's Progress**:
- Still continuing with Lists from "Automating the Boring Stuff with Python". This feels like a long chapter lol.
  - Reviewed tuples, and converting types with the list() and tuple() function
  - Clarified that variables do not necessarily store values, but they store 'references' to values. These references are stored in the computers memory (think a container or box)
  - Noted the id() function that returns the identity of the variable reference
  - Worth noting again, list methods like append() modify 'in-place'. So if my_list = ['cat', 'dog'] and has an ID of 123, if I run my_list.append('squirrel'), my_list is modified in place, and still contains the same reference ID of 123
- ** placeholder until I resume my studies. maybe after work
  - Studying has continued after dinner, where I learned that when using copy() and deepcopy() functions, you are creating an actual duplicate copy of a mutable value like a list; you are not just copying the reference of the list. Meaning the variable holding the original list, and the variable holding the copied list are independent of each other
  - Followed the code to create a 'Conway's Game of  Life' short program. Honestly it's over my head right now, but I'll have this saved away to refer to if needed.



**Thoughts:**
Unexpectedly woke up at 6 am, but instead of trying to force myself to go back to sleep, I decided to get up and have an early start to my studies.

This has been a goal for myself for a while to be able to wake up early and code/study before work. Here's hoping I can do this consistently and make it a habit.

**Thoughts: Late night**
I may make the decision to jump around in the chapters for Automating the Boring Stuff with Python. Conceptually, there's a lot of information, but pertaining to what I want to accomplish at my job, I don't think I need all of the details that are being depicted in some of the chapters.

I may start hopping around more towards chapters relating to actual automation, and then retroactively go back at chapters to understand any concepts. I think this will help it stick better.


**Link to work:**
- About Me: https://github.com/veroanalytic
- auto-boring-stuff repo: https://github.com/veroanalytic/auto-boring-stuff.git

---
### Day 2 | Thursday | September 8, 2022

**Today's Progress**:
- Updated my GitHub overview page with an About Me: README.md file. I used a generator to get the template, but will continue to alter it as needed.
- Still continuing with lists from "Automating the Boring Stuff with Python"
  - Reviewed list methods like append() and insert()
  - Also learned about the enumerate function
- ** putting a place holder here because I will resume studies after the gym :D
- Got back from the gym and continued more through my review of lists
  - practiced list methods such as remove(), sort(), passed keyword arguments within these methods like sort(reverse=True), to sort in descending order.
  - Went over sequence data types: lists, strings, range objects returned by the range() function and tuples are considered sequence data types. Meaning they can be indexed, sliced, and used in for loops with len() or with in and not in operators.


**Thoughts:**
I'm feeling impatient and want to get through these chapters that are of review for me and to the later chapters dealing with actual Python automation.

But I little by little, there are additional things that I'm picking up that I either forgot, or never learned during my initial pass at learning Python.

This is a marathon not a sprint, so I will continue to take it slower but with consistency.

**Thoughts after the gym:** I think I like this routine.
- Finish work
- Have dinner
- Study for a bit, digesting food, and record #66daysofdata challenge
- Go to gym
- Continue studying until tired

We'll see if I can keep this up.


**Link to work:**
- About Me: https://github.com/veroanalytic
- auto-boring-stuff repo: https://github.com/veroanalytic/auto-boring-stuff.git

---
### Day 1 | Wednesday | September 7, 2022

**Today's Progress**:
- Created 66-days-of-data repo to track progress on this challenge.
- Used Git to clone the remote repo down to my local repo. Pushing changes back to GitHub through Git as well.
- Continuing Chapter 4 of "Automating the Boring Stuff with Python", which is focused on lists.
  - Working with lists:
    - Created a variable and assigned it as an empty list []
    - Followed that with a while loop that breaks out of the loop of an empty string is entered
    - Otherwise it continues iterating and allows for more inputs which gets outputted in a for loop
    - File name 'allMyCats2_ch4.py' in the auto-boring-stuff.git repo

**Thoughts:**
I took quite a long break from my last round of #66daysofdata. I'm riding this wave of motivation to reestablish this as a habit. My primary goal for this round is to increase my skills with Python.

**Link to work:**
- auto-boring-stuff repo: https://github.com/veroanalytic/auto-boring-stuff.git
